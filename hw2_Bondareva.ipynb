{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hw2_Bondareva.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RYp0bXOFK-hP"
      },
      "source": [
        "# Машинное обучение, ФКН ВШЭ\n",
        "\n",
        "## Практическое задание 8. Метод опорных векторов и аппроксимация ядер\n",
        "\n",
        "### Общая информация\n",
        "Дата выдачи: 1.10.2021\n",
        "\n",
        "Мягкий дедлайн: 17.10.2021 23:59 МСК\n",
        "\n",
        "Жесткий дедлайн: 24.10.2021 23:59 МСК (1 неделя -- минус балл)\n",
        "\n",
        "### Оценивание и штрафы\n",
        "Каждая из задач имеет определенную «стоимость» (указана в скобках около задачи). Максимальная оценка за работу (без учёта бонусов) — 10 баллов.\n",
        "\n",
        "Сдавать задание после указанного жёсткого срока сдачи нельзя. При выставлении неполного балла за задание в связи с наличием ошибок на усмотрение проверяющего предусмотрена возможность исправить работу на указанных в ответном письме условиях.\n",
        "\n",
        "Задание выполняется самостоятельно. «Похожие» решения считаются плагиатом и все задействованные студенты (в том числе те, у кого списали) не могут получить за него больше 0 баллов (подробнее о плагиате см. на странице курса). Если вы нашли решение какого-то из заданий (или его часть) в открытом источнике, необходимо указать ссылку на этот источник в отдельном блоке в конце вашей работы (скорее всего вы будете не единственным, кто это нашел, поэтому чтобы исключить подозрение в плагиате, необходима ссылка на источник).\n",
        "\n",
        "Неэффективная реализация кода может негативно отразиться на оценке.\n",
        "\n",
        "### Формат сдачи\n",
        "Загрузите решение в свой репозиторий на github и поделитесь [ссылкой на решение в форме](https://forms.gle/ZzCaqRj6bmfpSpyL7). Не забудьте дать доступ к Вашему репозиторию, что у преподавателей была возмоожность проверить работу."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vY8vT0W_K-hR"
      },
      "source": [
        "### О задании\n",
        "\n",
        "На занятиях мы подробно обсуждали метод опорных векторов (SVM). В базовой версии в нём нет чего-то особенного — мы всего лишь используем специальную функцию потерь, которая не требует устремлять отступы к бесконечности; ей достаточно, чтобы отступы были не меньше +1. Затем мы узнали, что SVM можно переписать в двойственном виде, который, позволяет заменить скалярные произведения объектов на ядра. Это будет соответствовать построению модели в новом пространстве более высокой размерности, координаты которого представляют собой нелинейные модификации исходных признаков.\n",
        "\n",
        "Ядровой SVM, к сожалению, довольно затратен по памяти (нужно хранить матрицу Грама размера $d \\times d$) и по времени (нужно решать задачу условной оптимизации с квадратичной функцией, а это не очень быстро). Мы обсуждали, что есть способы посчитать новые признаки $\\tilde \\varphi(x)$ на основе исходных так, что скалярные произведения этих новых $\\langle \\tilde \\varphi(x), \\tilde \\varphi(z) \\rangle$ приближают ядро $K(x, z)$.\n",
        "\n",
        "Мы будем исследовать аппроксимации методом Random Fourier Features (RFF, также в литературе встречается название Random Kitchen Sinks) для гауссовых ядер. Будем использовать формулы, которые немного отличаются от того, что было на лекциях (мы добавим сдвиги внутрь тригонометрических функций и будем использовать только косинусы, потому что с нужным сдвигом косинус превратится в синус):\n",
        "$$\\tilde \\varphi(x) = (\n",
        "\\cos (w_1^T x + b_1),\n",
        "\\dots,\n",
        "\\cos (w_n^T x + b_n)\n",
        "),$$\n",
        "где $w_j \\sim \\mathcal{N}(0, 1/\\sigma^2)$, $b_j \\sim U[-\\pi, \\pi]$.\n",
        "\n",
        "На новых признаках $\\tilde \\varphi(x)$ мы будем строить любую линейную модель.\n",
        "\n",
        "Можно считать, что это некоторая новая парадигма построения сложных моделей. Можно направленно искать сложные нелинейные закономерности в данных с помощью градиентного бустинга или нейронных сетей, а можно просто нагенерировать большое количество случайных нелинейных признаков и надеяться, что быстрая и простая модель (то есть линейная) сможет показать на них хорошее качество. В этом задании мы изучим, насколько работоспособна такая идея.\n",
        "\n",
        "### Алгоритм\n",
        "\n",
        "Вам потребуется реализовать следующий алгоритм:\n",
        "1. Понизить размерность выборки до new_dim с помощью метода главных компонент.\n",
        "2. Для полученной выборки оценить гиперпараметр $\\sigma^2$ с помощью эвристики (рекомендуем считать медиану не по всем парам объектов, а по случайному подмножеству из где-то миллиона пар объектов): $$\\sigma^2 = \\text{median}_{i, j = 1, \\dots, \\ell, i \\neq j} \\left\\{\\sum_{k = 1}^{d} (x_{ik} - x_{jk})^2 \\right\\}$$\n",
        "3. Сгенерировать n_features наборов весов $w_j$ и сдвигов $b_j$.\n",
        "4. Сформировать n_features новых признаков по формулам, приведённым выше.\n",
        "5. Обучить линейную модель (логистическую регрессию или SVM) на новых признаках.\n",
        "6. Повторить преобразования (PCA, формирование новых признаков) к тестовой выборке и применить модель."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N_sGunb7K-hS"
      },
      "source": [
        "Тестировать алгоритм мы будем на данных Fashion MNIST. Ниже код для их загрузки и подготовки."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YyG6dBfjK-hS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97768998-5732-4d9a-9a72-38b35c06fb1a"
      },
      "source": [
        "import keras\n",
        "from keras.datasets import fashion_mnist\n",
        "(x_train_pics, y_train), (x_test_pics, y_test) = fashion_mnist.load_data()\n",
        "x_train = x_train_pics.reshape(x_train_pics.shape[0], -1)\n",
        "x_test = x_test_pics.reshape(x_test_pics.shape[0], -1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "32768/29515 [=================================] - 0s 0us/step\n",
            "40960/29515 [=========================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26427392/26421880 [==============================] - 0s 0us/step\n",
            "26435584/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "16384/5148 [===============================================================================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4423680/4422102 [==============================] - 0s 0us/step\n",
            "4431872/4422102 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rJNN55F7K-hT"
      },
      "source": [
        "__Задание 1. (5 баллов)__\n",
        "\n",
        "Реализуйте алгоритм, описанный выше. Можете воспользоваться шаблоном класса ниже или написать свой интерфейс.\n",
        "\n",
        "Ваша реализация должна поддерживать следующие опции:\n",
        "1. Возможность задавать значения гиперпараметров new_dim (по умолчанию 50) и n_features (по умолчанию 1000).\n",
        "2. Возможность включать или выключать предварительное понижение размерности с помощью метода главных компонент.\n",
        "3. Возможность выбирать тип линейной модели (логистическая регрессия или SVM с линейным ядром).\n",
        "\n",
        "Протестируйте на данных Fashion MNIST, сформированных кодом выше. Если на тесте у вас получилась доля верных ответов не ниже 0.84 с гиперпараметрами по умолчанию, то вы всё сделали правильно."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iSnaIfsdwJ-C",
        "outputId": "7f091554-7764-4e99-8cd4-813cba4d4f5d"
      },
      "source": [
        "!pip install catboost"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting catboost\n",
            "  Downloading catboost-1.0.0-cp37-none-manylinux1_x86_64.whl (76.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 76.4 MB 1.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from catboost) (3.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from catboost) (1.4.1)\n",
            "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.7/dist-packages (from catboost) (1.1.5)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.7/dist-packages (from catboost) (0.10.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from catboost) (1.15.0)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.7/dist-packages (from catboost) (4.4.1)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from catboost) (1.19.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->catboost) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->catboost) (2018.9)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->catboost) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->catboost) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->catboost) (1.3.2)\n",
            "Requirement already satisfied: retrying>=1.3.3 in /usr/local/lib/python3.7/dist-packages (from plotly->catboost) (1.3.3)\n",
            "Installing collected packages: catboost\n",
            "Successfully installed catboost-1.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jP8yepx8K-hT"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.linear_model import LogisticRegression \n",
        "from sklearn.svm import SVC\n",
        "import catboost\n",
        "\n",
        "\n",
        "class RFFPipeline(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, n_features=1000, new_dim=50, use_PCA=True, classifier='logreg'):\n",
        "        \"\"\"        \n",
        "        Implements pipeline, which consists of PCA decomposition,\n",
        "        Random Fourier Features approximation and linear classification model.\n",
        "        \n",
        "        n_features, int: amount of synthetic random features generated with RFF approximation.\n",
        "\n",
        "        new_dim, int: PCA output size.\n",
        "        \n",
        "        use_PCA, bool: whether to include PCA preprocessing.\n",
        "        \n",
        "        classifier, string: either 'svm' or 'logreg', a linear classification model to use on top of pipeline.\n",
        "        \n",
        "        Feel free to edit this template for your preferences.    \n",
        "        \"\"\"\n",
        "        self.n_features = n_features\n",
        "        self.use_PCA = use_PCA\n",
        "        self.new_dim = new_dim\n",
        "        self.classifier = classifier\n",
        "        \n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "        Fit all parts of algorithm (PCA, RFF, Classification) to training set.\n",
        "        \"\"\"\n",
        "        # Your code here: (￣▽￣)/♫•*¨*•.¸¸♪\n",
        "        if self.use_PCA:\n",
        "            self.pca = PCA(n_components=self.new_dim)\n",
        "            X = self.pca.fit_transform(X)\n",
        "\n",
        "        random_element1, random_element2 = np.random.choice(range(len(X)), 1000000), \\\n",
        "                                           np.random.choice(range(len(X)), 1000000)\n",
        "        mask = random_element1 != random_element2\n",
        "        random_element1, random_element2 = X[random_element1[mask]], \\\n",
        "                                           X[random_element2[mask]]\n",
        "        sigma = np.median(np.sum((random_element1-random_element2)**2, axis=1))\n",
        "        self.w = np.random.normal(0, np.sqrt(1/sigma), (self.n_features, X.shape[1]))\n",
        "        self.b = np.random.uniform(-np.pi, np.pi, self.n_features)\n",
        "        new_X = np.cos(X.dot(self.w.T) + self.b)\n",
        "        if self.classifier=='logreg':\n",
        "            self.model = LogisticRegression()\n",
        "        elif self.classifier=='svm':\n",
        "            self.model = SVC(kernel='linear')\n",
        "        self.model.fit(new_X, y)\n",
        "        \n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        \"\"\"\n",
        "        Apply pipeline to obtain scores for input data.\n",
        "        \"\"\"\n",
        "        # Your code here: (￣▽￣)/♫•*¨*•.¸¸♪\n",
        "        if self.use_PCA:\n",
        "            X = self.pca.transform(X)\n",
        "        new_X = np.cos(X.dot(self.w.T) + self.b)\n",
        "        return self.model.predict_proba(new_X)\n",
        "        \n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Apply pipeline to obtain discrete predictions for input data.\n",
        "        \"\"\"\n",
        "        # Your code here: (￣▽￣)/♫•*¨*•.¸¸♪\n",
        "        if self.use_PCA:\n",
        "            X = self.pca.transform(X)\n",
        "        new_X = np.cos(X.dot(self.w.T) + self.b)\n",
        "        return self.model.predict(new_X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uC6fM0qYR4Te",
        "outputId": "3fab9807-5773-4d00-a05e-8a1d19f2dc85"
      },
      "source": [
        "%%time\n",
        "rff_logreg = RFFPipeline()\n",
        "rff_logreg.fit(x_train, y_train)\n",
        "preds_logreg = rff_logreg.predict(x_test)\n",
        "print(f'Accuracy: {np.sum(preds_logreg==y_test)/len(preds_logreg)}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.8576\n",
            "CPU times: user 1min 55s, sys: 20.3 s, total: 2min 15s\n",
            "Wall time: 1min 11s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HYqQUEi-K-hU"
      },
      "source": [
        "__Задание 2. (3 балла)__\n",
        "\n",
        "Сравните подход со случайными признаками с обучением SVM на исходных признаках. Попробуйте вариант с обычным (линейным) SVM и с ядровым SVM. Ядровой SVM может очень долго обучаться, поэтому можно делать любые разумные вещи для ускорения: брать подмножество объектов из обучающей выборки, например.\n",
        "\n",
        "Сравните подход со случайными признаками с вариантом, в котором вы понижаете размерность с помощью PCA и обучаете градиентный бустинг. Используйте одну из реализаций CatBoost/LightGBM/XGBoost, не забудьте подобрать число деревьев и длину шага.\n",
        "\n",
        "Сделайте выводы — насколько идея со случайными признаками работает? Сравните как с точки зрения качества, так и с точки зрения скорости обучения и применения."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Iu25blrXkX1"
      },
      "source": [
        "Сравнивать качество буду на подмножестве из 10000 наблюдений обучающей выборки, дабы ускорить обучение"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fLU3S-32ZxyL"
      },
      "source": [
        "random_idx = np.random.choice(range(len(x_train)), 10000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qN8LUlJgK-hV",
        "outputId": "18bde3b9-324d-42a0-cb99-7138bc2c96fb"
      },
      "source": [
        "%%time\n",
        "lSVC = SVC(kernel='linear')\n",
        "lSVC.fit(x_train[random_idx], y_train[random_idx])\n",
        "preds = lSVC.predict(x_test)\n",
        "print(f'Accuracy: {np.sum(preds==y_test)/len(preds)}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.7992\n",
            "CPU times: user 1min 2s, sys: 72 ms, total: 1min 2s\n",
            "Wall time: 1min 2s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jz3_iPR2YtGF",
        "outputId": "0df2bbe9-c28b-4432-a326-3ee84e9336ff"
      },
      "source": [
        "%%time\n",
        "rSVC = SVC(kernel='rbf')\n",
        "rSVC.fit(x_train[random_idx], y_train[random_idx])\n",
        "preds = rSVC.predict(x_test)\n",
        "print(f'Accuracy: {np.sum(preds==y_test)/len(preds)}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.851\n",
            "CPU times: user 1min 10s, sys: 69.3 ms, total: 1min 10s\n",
            "Wall time: 1min 9s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DtDhmHTKcir-",
        "outputId": "bc79c66f-b86f-4965-990a-0c855869f35a"
      },
      "source": [
        "%%time\n",
        "rff_logreg = RFFPipeline()\n",
        "rff_logreg.fit(x_train[random_idx], y_train[random_idx])\n",
        "preds_logreg = rff_logreg.predict(x_test)\n",
        "print(f'Accuracy: {np.sum(preds_logreg==y_test)/len(preds_logreg)}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.8461\n",
            "CPU times: user 20.3 s, sys: 13.2 s, total: 33.5 s\n",
            "Wall time: 17.6 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "br0tW7nHSYq2",
        "outputId": "44826997-620c-4c6a-edca-e2072cd9b9a9"
      },
      "source": [
        "%%time\n",
        "rff_svc = RFFPipeline(classifier='svm')\n",
        "rff_svc.fit(x_train[random_idx], y_train[random_idx])\n",
        "preds_svc = rff_svc.predict(x_test)\n",
        "print(f'Accuracy: {np.sum(preds_svc==y_test)/len(preds_svc)}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.8454\n",
            "CPU times: user 1min 1s, sys: 1.77 s, total: 1min 2s\n",
            "Wall time: 1min\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4tL-W6dneN4-",
        "outputId": "e1102b3a-1878-4b15-d7ab-0b66fad1bfed"
      },
      "source": [
        "# гиперпараметры подбирала вручную\n",
        "%%time\n",
        "pca = PCA(n_components=50)\n",
        "new_X = pca.fit_transform(x_train[random_idx])\n",
        "model = catboost.CatBoostClassifier(random_state=42, verbose=False, depth=6, \n",
        "                                    l2_leaf_reg=20, n_estimators=500, learning_rate=0.02,\n",
        "                                    bagging_temperature=20)\n",
        "model.fit(new_X, y_train[random_idx])\n",
        "preds = model.predict(pca.transform(x_test))[:, 0]\n",
        "print(f'Accuracy: {np.sum(preds==y_test)/len(preds)}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.8055\n",
            "CPU times: user 2min 21s, sys: 2.06 s, total: 2min 23s\n",
            "Wall time: 1min 13s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1plFnES6YuXk"
      },
      "source": [
        "Результаты работы моделей обучала на случайном подмножестве обучающей выборки. В целом можно сказать, что использование RFF улучшает метрику качества в текущей задаче. Влияние расчета RFF на время выполнения минимально, а качество растет. \n",
        "\n",
        "Таким образом, использование RFF вместе с Linear SVC существенно улучшает качество модели."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6umjhWuK-hV"
      },
      "source": [
        "__Задание 3. (2 балла)__\n",
        "\n",
        "Проведите эксперименты:\n",
        "1. Помогает ли предварительное понижение размерности с помощью PCA? \n",
        "2. Как зависит итоговое качество от n_features? Выходит ли оно на плато при росте n_features?\n",
        "3. Важно ли, какую модель обучать — логистическую регрессию или SVM?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c2QIHIMbK-hW"
      },
      "source": [
        "# Your code here: (￣▽￣)/♫•*¨*•.¸¸♪"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ld4ktwmRfW93",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06bd922f-7851-4db7-e055-44248324f55a"
      },
      "source": [
        "%%time\n",
        "rff_svc = RFFPipeline(classifier='svm', use_PCA=False)\n",
        "rff_svc.fit(x_train[random_idx], y_train[random_idx])\n",
        "preds_svc = rff_svc.predict(x_test)\n",
        "print(f'Accuracy без PCA: {np.sum(preds_svc==y_test)/len(preds_svc)}')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy без PCA: 0.0962\n",
            "CPU times: user 12min 50s, sys: 917 ms, total: 12min 50s\n",
            "Wall time: 12min 47s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Rj9dJBzbLu9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d723db40-437c-4588-f733-f574f86f4495"
      },
      "source": [
        "%%time\n",
        "rff_lr = RFFPipeline(use_PCA=False)\n",
        "rff_lr.fit(x_train[random_idx], y_train[random_idx])\n",
        "preds_lr = rff_lr.predict(x_test)\n",
        "print(f'Accuracy без PCA: {np.sum(preds_lr==y_test)/len(preds_lr)}')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy без PCA: 0.1041\n",
            "CPU times: user 7.67 s, sys: 2.4 s, total: 10.1 s\n",
            "Wall time: 6.31 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XBET8hGZNW1b"
      },
      "source": [
        "Если не использовать PCA перед RFF, то хорошего качества не получим. Это обусловлено тем, что с помощью PCA мы получаем максимально полезную информацию из признаков в новые признаки."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JSSKYtPPaY4F"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gpzoSH58baTS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aab0cecd-d184-4d4d-a774-a3c70d4a9dc1"
      },
      "source": [
        "for n in [10, 50, 100, 250, 500, 1000, 2000, 3000]:\n",
        "    rff_svc = RFFPipeline(classifier='svm', n_features=n)\n",
        "    rff_svc.fit(x_train[random_idx], y_train[random_idx])\n",
        "    preds_svc = rff_svc.predict(x_test)\n",
        "    print(f'Accuracy SVC c n_features={n}: {np.sum(preds_svc==y_test)/len(preds_svc)}')\n",
        "    rff_lr = RFFPipeline(n_features=n)\n",
        "    rff_lr.fit(x_train[random_idx], y_train[random_idx])\n",
        "    preds_lr = rff_lr.predict(x_test)\n",
        "    print(f'Accuracy LogReg c n_features={n}: {np.sum(preds_lr==y_test)/len(preds_lr)}')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy SVC c n_features=10: 0.6695\n",
            "Accuracy LogReg c n_features=10: 0.6595\n",
            "Accuracy SVC c n_features=50: 0.8121\n",
            "Accuracy LogReg c n_features=50: 0.8024\n",
            "Accuracy SVC c n_features=100: 0.8337\n",
            "Accuracy LogReg c n_features=100: 0.8238\n",
            "Accuracy SVC c n_features=250: 0.8438\n",
            "Accuracy LogReg c n_features=250: 0.8368\n",
            "Accuracy SVC c n_features=500: 0.8482\n",
            "Accuracy LogReg c n_features=500: 0.8429\n",
            "Accuracy SVC c n_features=1000: 0.8494\n",
            "Accuracy LogReg c n_features=1000: 0.8436\n",
            "Accuracy SVC c n_features=2000: 0.846\n",
            "Accuracy LogReg c n_features=2000: 0.8436\n",
            "Accuracy SVC c n_features=3000: 0.8457\n",
            "Accuracy LogReg c n_features=3000: 0.849\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hTt1YW2gN8sy"
      },
      "source": [
        "Качество выходит на плато после n_features=500,1000. Но если n_features ниже 500, то качество заметно хуже. В целом вид модели особо не влияет на качество, оно примерно одинаково."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LF35pq-Fz1dD"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CJqXVuasK-hW"
      },
      "source": [
        "### Бонус"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QVDWHCdrK-hX"
      },
      "source": [
        "__Задание 4. (Максимум 2 балла)__\n",
        "\n",
        "Как вы, должно быть, помните с курса МО-1, многие алгоритмы машинного обучения работают лучше, если признаки данных некоррелированы. Оказывается, что для RFF существует модификация, позволяющая получать ортогональные случайные признаки (Orthogonal Random Features, ORF). Об этом методе можно прочитать в [статье](https://proceedings.neurips.cc/paper/2016/file/53adaf494dc89ef7196d73636eb2451b-Paper.pdf). Реализуйте класс для вычисления ORF по аналогии с основным заданием. Обратите внимание, что ваш класс должен уметь работать со случаем n_features > new_dim (в статье есть замечание на этот счет). Проведите эксперименты, сравнивающие RFF и ORF, сделайте выводы."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HSxvGI9iK-hX"
      },
      "source": [
        "# Your code here: (￣▽￣)/♫•*¨*•.¸¸♪"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4pc7-1jmK-hY"
      },
      "source": [
        "__Задание 5. (Максимум 2 балла)__\n",
        "\n",
        "Поэкспериментируйте с функциями для вычисления новых случайных признаков. Не обязательно использовать косинус от скалярного произведения — можно брать знак от него, хэш и т.д. Придумайте побольше вариантов для генерации признаков и проверьте, не получается ли с их помощью добиваться более высокого качества. Также можете попробовать другой классификатор поверх случайных признаков, сравните результаты."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dWj-O2vjK-hY"
      },
      "source": [
        "# Your code here: (￣▽￣)/♫•*¨*•.¸¸♪"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}